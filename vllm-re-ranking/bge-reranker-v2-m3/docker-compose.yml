services:
  chat-model:
    image: vllm/vllm-openai:latest
    shm_size: '64gb'
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0', '1' ]
              capabilities: [ gpu ]
    env_file:
      - ../../.env
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - CUDA_VISIBLE_DEVICES=0,1
      - port=${port}
    healthcheck:
      test: [ "CMD", "curl", "-H", "Authorization: Bearer re-ranking-apikey", "-f", "http://0.0.0.0:6000/v1/models" ]
      interval: 30s
      timeout: 5s
      retries: 20
    ports:
      - "${port}:6000"
    volumes:
      - "~/.cache/huggingface:/root/.cache/huggingface"
    ipc: "host"

    entrypoint: vllm
    command: serve --task auto "BAAI/bge-reranker-v2-m3" --dtype auto --port 6000 --tensor-parallel-size 2 --gpu-memory-utilization=${gpu_memory_utilization} --max-num-batched-tokens 8194  --max_num_seqs 2 --api-key re-ranking-apikey

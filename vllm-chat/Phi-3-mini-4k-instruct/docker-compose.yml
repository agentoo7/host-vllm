services:
  chat-model:
      image: vllm/vllm-openai:latest
      shm_size: '64gb'
      runtime: nvidia
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                device_ids: ['0', '1']
                capabilities: [gpu]
      env_file:
        - .env
      environment:
        - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
        - CUDA_VISIBLE_DEVICES=0,1
      healthcheck:
        test: [ "CMD", "curl", "-f", "http://0.0.0.0:6000/v1/models" ]
        interval: 30s
        timeout: 5s
        retries: 20
      ports:
        - "6000:6000"
      volumes:
        - "~/.cache/huggingface:/root/.cache/huggingface"
      ipc: "host"
      
      entrypoint: vllm
      # 8192: max_model_len (4096) * 2
      command: serve "microsoft/Phi-3-mini-4k-instruct" --dtype auto --port 6000 --tensor-parallel-size 2 --gpu-memory-utilization 0.50 --max-num-batched-tokens 8192  --max_num_seqs 2 --api-key chat-apikey
      # command: serve "microsoft/phi-4" --dtype auto --port 6000 --tensor-parallel-size 2 --gpu-memory-utilization 0.50 --max-num-batched-tokens 16384  --max_num_seqs 2 --api-key chat-apikey
      
  
  